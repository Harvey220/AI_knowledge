# 现代深度学习的优化器

| 优化器                        | 核心思想（原理差异）                                         | 优点                                     | 缺点                         | 常用领域                    |
| ----------------------------- | ------------------------------------------------------------ | ---------------------------------------- | ---------------------------- | --------------------------- |
| **SGD + Momentum / Nesterov** | 用“当前梯度 + 惯性”更新参数。Nesterov 会“提前预判”下一步的梯度。 | 泛化强（测试集表现好）、稳定、经典       | 收敛速度慢，需要仔细调学习率 | CNN、图像任务               |
| **Adam**                      | Momentum + 自适应学习率（每个参数都有自己的学习率，会根据历史梯度自动调节大小） | 收敛快、默认超参好用                     | 泛化有时不如 SGD             | NLP、对比学习、一般深度学习 |
| **AdamW**                     | 是 Adam 的改进版：把权重衰减从 Adam 公式中“分离”出来，修复了 Adam 的正则问题 → 更稳定、更泛化 | Transformer / 大模型标配，泛化比 Adam 好 | 超参数稍多（但默认值很好）   | Transformer、LLM、BERT、ViT |

CV 用 SGD+Momentum，NLP 用 AdamW，其他任务 Adam 足够。